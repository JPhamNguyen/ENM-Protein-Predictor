"""Developed by: Matthew Findlay 2017

This module includes a class with several validation metrics and helper
functions for those validation metrics.
"""

import data_utils
import numpy as np
import pandas as pd
from sklearn.exceptions import UndefinedMetricWarning
import math
import visualization_utils
import sklearn
from sklearn import metrics
import sys
import statistics


class validation_metrics(object):
    """Several statistical tests used to validate the models predictive power

    Args:
        :param true_results (array of ints): The true testing values
        :param predicted_results (array of floats): The predicted probabilities
        generated by the model
    Attributes:
        :true_results (array of ints): The true testing values
        :predicted_results (array of floats): The predicted probabilities
        generated by the model
        : mse (array of floats): The mse values
        :rmse (array of floats): The rmse values
        :mae (array of floats): The mae values
        : mape (array of floats): The mape values
    """
    def __init__(self):
        self._true_results = None
        self._predicted_results = None
        self._mse = []
        self._rmse = []
        self._mae = []
        self._mape = []

    def set_parameters(self, true_results, predicted_results):
        self._true_results = true_results
        self._predicted_results = predicted_results

    def calculate_error_metrics(self):
        """Calculate various error metric values using several regression metrics like MSE, RMSE, Huber Loss,
        and store them into error-metric-specific lists

        Args, Returns: None
        """
        # as the model goes through the pipeline, store each individual error score in each respective list
        self._mse.append(metrics.mean_squared_error(y_true=self._true_results, y_pred=self._predicted_results))
        self._rmse.append(metrics.mean_squared_error(y_true=self._true_results, y_pred=self._predicted_results, squared=False))
        self._mae.append(metrics.mean_absolute_error(y_true=self._true_results, y_pred=self._predicted_results))
        self._mape.append(metrics.mean_absolute_percentage_error(y_true=self._true_results, y_pred=self._predicted_results))
        print("MSE value: {}\n".format(metrics.mean_squared_error(y_true=self._true_results, y_pred=self._predicted_results)))
        print("RSME value: {}\n".format(metrics.mean_squared_error(y_true=self._true_results, y_pred=self._predicted_results, squared=False)))
        print("MAE value: {}\n".format(metrics.mean_absolute_error(y_true=self._true_results, y_pred=self._predicted_results)))
        print("MAPE value: {}\n".format(metrics.mean_absolute_percentage_error(y_true=self._true_results, y_pred=self._predicted_results)))

    def average_error_metrics(self):
        """After the pipeline has been run for 'x' amount of times, take the average of each error metric to report

        Args: None
        Returns:
            :return: avg_errs (dict): a dictionary containing key-value pairs in the form {'type of error': error value}
        """
        keys = ['average MSE', 'average RMSE', 'average MAE', 'average MAPE']
        avg_errs_vals = [statistics.mean(self._mse), statistics.mean(self._rmse), statistics.mean(self._mae),
                         statistics.mean(self._mape)]
        avg = dict(zip(keys, avg_errs_vals))
        print("This is the average of the errors: \n{}".format(avg))
        return dict(zip(keys, avg_errs_vals))


