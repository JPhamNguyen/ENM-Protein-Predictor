"""Developed by: Matthew Findlay 2017
Modified by: Joseph Pham 2021

This module calculates and stores error metrics and other useful statistics for measuring model performance
"""

from sklearn import metrics
import statistics as stat
import sys
import pandas as pd


class validation_metrics(object):
    """Several statistical tests used to validate the models predictive power

    Args:
        :param: true_results (array of ints): The true testing values
        :param: predicted_results (array of floats): The predicted probabilities
        generated by the model
        :param: predicted_results_by_acession_number (dict of floats): dictionary that contains accession-number-
        specific values
    Attributes:
        :true_results (array of ints): The true testing values
        :predicted_results (array of floats): The predicted probabilities
        generated by the model
        : mse (array of floats): The mse values
        :rmse (array of floats): The rmse values
        :mae (array of floats): The mae values
        : mape (array of floats): The mape values
    """
    def __init__(self):
        self._true_results = None
        self._predicted_results = None
        self._sorted_predicted_results = {}
        self._feature_importances = {}
        self._mse = []
        self._rmse = []
        self._mae = []
        self._mape = []

    def set_parameters(self, true_results, predicted_results, accession_numbers):
        """During the pipeline, set temporary true_results and predicted results to calculate error metrics and
        other desired information
        Args:
            :param: true_results (): array containing the true targets to compare prediction values against
            :param: predicted_results (ndarray of floats): contains predicted targets for prediction on X_test
        """
        # append the accession numbers as an index to the true_results for further updates
        self._true_results = pd.DataFrame(true_results, columns=['True Value'], index=accession_numbers)
        print(self._true_results)
        sys.exit(0)
        self._predicted_results = predicted_results.tolist()

    def update_feature_importances(self, optimal_features, scores):
        """Update the feature_importance scores and store into a dictionary
        Args:
            :param: optimal_features (pandas Series): contains the names of optimal features inputted into model
            :param: scores (narray of floats): contains the Gini importance scores for each optimal feature
        Returns: None
        """
        for idx, feat in enumerate(optimal_features):
            self._feature_importances.setdefault(feat, []).append(scores[idx])

    def update_predictions(self, accession_numbers):
        """Update predicted values by accession number
        Args:
            :param: accession_numbers (pandas Series): Series containing the tested particle protein pairs' Accession Numbers
            :param: y_pred (array of floats): nparray containing the predicted regression targets for X_test
        Returns: None
        """
        # append new predicted values to each specific accession number key
        for idx, a_num in enumerate(accession_numbers):
            self._sorted_predicted_results.setdefault(a_num, {'Min': None, 'Max': None, 'Standard Deviation': None,
                                                              'True Value': [], 'Average Predicted Value': []})
            # input single ground truth value
            # if self._sorted_predicted_results[a_num]['True Value'] is not None:
            #    self._sorted_predicted_results[a_num]['True Value'] =
            self._sorted_predicted_results[a_num]['Average Predicted Value'].append(self._predicted_results[idx])

    def calculate_error_metrics(self):
        """Calculate various error metric values like MSE and RMSE, and store them into error-metric-specific lists
        Args, Returns: None
        """
        # as the model goes through the pipeline, store each individual error score in each respective list
        self._mse.append(metrics.mean_squared_error(y_true=self._true_results, y_pred=self._predicted_results))
        self._rmse.append(metrics.mean_squared_error(y_true=self._true_results, y_pred=self._predicted_results, squared=False))
        self._mae.append(metrics.mean_absolute_error(y_true=self._true_results, y_pred=self._predicted_results))
        self._mape.append(metrics.mean_absolute_percentage_error(y_true=self._true_results, y_pred=self._predicted_results))

    def calculate_final_metrics(self):
        """After the pipeline has been run for 'x' amount of times, calculate final metrics and return them as a dictionary
        Args: None
        Returns:
            :return: avg_errs (dict): a dictionary containing key-value pairs in the form {'type of error': error value}
            :return: sorted_predicted_results: dictionary containing the average of each accession number-predicted pair
            :return: feature_importances: dictionary containing the average of the Gini importances for each optimal feature
        """
        # calculate the average of all error metrics
        keys = ['average MSE', 'average RMSE', 'average MAE', 'average MAPE']
        avg_errs_vals = [stat.mean(self._mse), stat.mean(self._rmse), stat.mean(self._mae),
                         stat.mean(self._mape)]

        # calculate average of predicted values for each accession number-protein pair
        for a_num in self._sorted_predicted_results.keys():
            self._sorted_predicted_results[a_num]['Min'] = min(self._sorted_predicted_results[a_num]
                                                               ['Average Predicted Value'])
            self._sorted_predicted_results[a_num]['Max'] = max(self._sorted_predicted_results[a_num]
                                                               ['Average Predicted Value'])
            try:
                self._sorted_predicted_results[a_num]['Standard Deviation'] = stat.stdev(self._sorted_predicted_results
                                                                                     [a_num]['Average Predicted Value'])
            except stat.StatisticsError:
                self._sorted_predicted_results[a_num]['Standard Deviation'] = 0.0

            self._sorted_predicted_results[a_num]['Average Predicted Value'] = stat.mean(self._sorted_predicted_results
                                                                                         [a_num]['Average Predicted Value'])

        # calculate average of feature importances
        for feat in self._feature_importances.keys():
            self._feature_importances[feat] = stat.mean(self._feature_importances[feat])

        return dict(zip(keys, avg_errs_vals)), self._sorted_predicted_results, self._feature_importances


