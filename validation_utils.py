"""Developed by: Matthew Findlay 2017
Modified by: Joseph Pham 2021

This module calculates and stores error metrics and other useful statistics for measuring model performance
"""
import statistics

import data_utils
import numpy as np
import pandas as pd
import math
import visualization_utils
import sklearn
from sklearn import metrics
import sys
import statistics as stat


class validation_metrics(object):
    """Several statistical tests used to validate the models predictive power

    Args:
        :param: true_results (array of ints): The true testing values
        :param: predicted_results (array of floats): The predicted probabilities
        generated by the model
        :param: predicted_results_by_acession_number (dict of floats): dictionary that contains accession-number-
        specific values
    Attributes:
        :true_results (array of ints): The true testing values
        :predicted_results (array of floats): The predicted probabilities
        generated by the model
        : mse (array of floats): The mse values
        :rmse (array of floats): The rmse values
        :mae (array of floats): The mae values
        : mape (array of floats): The mape values
    """
    def __init__(self):
        self._true_results = None
        self._predicted_results = None
        self._sorted_predicted_results = {}
        self._feature_importances = {}
        self._mse = []
        self._rmse = []
        self._mae = []
        self._mape = []

    def set_parameters(self, true_results, predicted_results):
        """During the pipeline, set temporary true_results and predicted results to calculate error metrics and
        other desired information
        Args:
            :param: true_results (): array containing the true targets to compare prediction values against
            :param: predicted_results (ndarray of floats): contains predicted targets for prediction on X_test
        """
        self._true_results = true_results
        self._predicted_results = predicted_results.tolist()

    def update_feature_importances(self, optimal_features, scores):
        """Update the feature_importance scores and store into a dictionary
        Args:
            :param: optimal_features (pandas Series): contains the names of optimal features inputted into model
            :param: scores (narray of floats): contains the Gini importance scores for each optimal feature
        Returns: None
        """
        for idx, feat in enumerate(optimal_features):
            self._feature_importances.setdefault(feat, []).append(scores[idx])

    def update_predictions_by_accession_number(self, accession_numbers):
        """Append new predicted values by Accession Number + classified particle protein pairs. Each time it does so,
        calculate the new average predicted all the way until the number of iterations on the model is done
        Args:
            :param: accession_numbers (pandas Series): Series containing the tested particle protein pairs' Accession Numbers
            :param: y_pred (array of floats): nparray containing the predicted regression targets for X_test
        Returns: None
        """
        # append new predicted values to each specific accession number key
        for idx, a_num in enumerate(accession_numbers):
            # self._sorted_predicted_results.setdefault(a_num, []).append(self._predicted_results[idx])
            self._sorted_predicted_results.setdefault(a_num, {'Min': None, 'Max': None, 'Standard Deviation': None,
                                                              'Average Predicted Value': []})
            self._sorted_predicted_results[a_num]['Average Predicted Value'].append(self._predicted_results[idx])

    def calculate_error_metrics(self):
        """Calculate various error metric values using several regression metrics like MSE, RMSE, Huber Loss,
        and store them into error-metric-specific lists
        Args, Returns: None
        """
        # as the model goes through the pipeline, store each individual error score in each respective list
        self._mse.append(metrics.mean_squared_error(y_true=self._true_results, y_pred=self._predicted_results))
        self._rmse.append(metrics.mean_squared_error(y_true=self._true_results, y_pred=self._predicted_results, squared=False))
        self._mae.append(metrics.mean_absolute_error(y_true=self._true_results, y_pred=self._predicted_results))
        self._mape.append(metrics.mean_absolute_percentage_error(y_true=self._true_results, y_pred=self._predicted_results))

    def calculate_final_metrics(self):
        """After the pipeline has been run for 'x' amount of times, calculate final metrics and return them as a dictionary
        Args: None
        Returns:
            :return: avg_errs (dict): a dictionary containing key-value pairs in the form {'type of error': error value}
            :return: sorted_predicted_results: dictionary containing the average of each accession number-predicted pair
            :return: feature_importances: dictionary containing the average of the Gini importances for each optimal feature
        """
        # calculate the average of all error metrics
        keys = ['average MSE', 'average RMSE', 'average MAE', 'average MAPE']
        avg_errs_vals = [stat.mean(self._mse), stat.mean(self._rmse), stat.mean(self._mae),
                         stat.mean(self._mape)]

        # calculate average of predicted values for each accession number-protein pair
        for a_num in self._sorted_predicted_results.keys():
            self._sorted_predicted_results[a_num]['Min'] = min(self._sorted_predicted_results[a_num]
                                                               ['Average Predicted Value'])
            self._sorted_predicted_results[a_num]['Max'] = max(self._sorted_predicted_results[a_num]
                                                               ['Average Predicted Value'])
            try:
                self._sorted_predicted_results[a_num]['Standard Deviation'] = stat.stdev(self._sorted_predicted_results
                                                                                     [a_num]['Average Predicted Value'])
            except stat.StatisticsError:
                self._sorted_predicted_results[a_num]['Standard Deviation'] = 0.0

            self._sorted_predicted_results[a_num]['Average Predicted Value'] = stat.mean(self._sorted_predicted_results
                                                                                         [a_num]['Average Predicted Value'])

        # calculate average of feature importances
        for feat in self._feature_importances.keys():
            self._feature_importances[feat] = stat.mean(self._feature_importances[feat])

        return dict(zip(keys, avg_errs_vals)), self._sorted_predicted_results, self._feature_importances


